<!DOCTYPE html>
<html>

    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Hugo Nicolau's personal webpage. Hugo is an Assistant Professor in the Computer Science and Engineering Department (DEI) of Instituto Superior Técnico, University of Lisbon in Portugal. He's also a researcher at the Visualization and Intelligent Multimodal Interfaces (VIMMI) group at INESC-ID. His research interests include human-computer interaction, accessibility, and user-centred design, focusing on the design, development, and evaluation of novel mobile and ubiquitous computing applications.">
    <meta name="author" content="Hugo Nicolau">
    <link rel="shortcut icon" href="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/ico/favicon.ico">

    <title>HUGO NICOLAU - Accessibility in the Wild</title>

    <!-- Bootstrap core CSS -->
    <link href="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/css/style.css" rel="stylesheet">
    <link href="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/css/font-awesome.min.css" rel="stylesheet">
    
</head>

    <body>
        <!-- Fixed navbar -->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://web.tecnico.ulisboa.pt/hugo.nicolau/">HUGO NICOLAU</a>
    </div>
    <div class="navbar-collapse collapse navbar-right">
      <ul class="nav navbar-nav">
        <li ><a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/">HOME</a></li>
        <li ><a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/">PUBLICATIONS</a></li>
        <li ><a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/aboutme/">ABOUT ME</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div>
</div>

            <div id="blue">
    <div class="container">
        <div class="row">
            <h3>Accessibility in the Wild</h3>
        </div><!-- /row -->
    </div> <!-- /container -->
</div><!-- /blue -->

<div class="container">
    <div class="row">

        <div class="col-lg-10 col-lg-offset-1 centered">
    <div id="carousel-example-generic" class="carousel slide" data-ride="carousel">
        <!-- Indicators -->
        <ol class="carousel-indicators">
            
                <li data-target="#carousel-example-generic" data-slide-to="0" 
                
                    class="active"
                >
                </li>
            
        </ol>

        <!-- Wrapper for slides -->
        <div class="carousel-inner">
            
                <div class="item active">
                    <img src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/img/project/carousel/accessibilitywild_01.jpg" alt="">
                </div>
            
            
        </div>
    </div> <!--/Carousel -->
</div>


        <div class="col-lg-5 col-lg-offset-1">
            <div class="spacing"></div>

            <p>Over the last decades there has been much research on mobile accessibility. However, most of it relates to laboratory settings, producing a snapshot of user performance. Understanding how performance changes over time and how people truly use their mobile devices remains an open question. Having a deeper knowledge of the challenges, frustrations, and overall true user experience is of upmost importance to improve current mobile technologies.</p>

<p>In this project, we are creating the tools and gathering the knowledge to characterize user performance in the wild (real-world) in order to improve current devices and interfaces that are used everyday.</p>


        </div>
        <div class="col-lg-4 col-lg-offset-1">
            <div class="spacing"></div>
            <h4>Project Details</h4>
            <div class="hline"></div>
            <p><b>Title:</b> Accessibility in the Wild</p>
            <p><b>Date:</b> Jan 1, 2017</p>
            <p><b>Authors:</b> André Rodrigues, Hugo Nicolau, Kyle Montague, Tiago Guerreiro, João Guerreiro</p>
            <!--<p><b>Categories:</b> project</p>-->
            <p><b>Keywords:</b> accessibility, mobile, laboratory, in-the-wild, everyday, touchscreen, mobile, performance</p>
            <!--<p><b>Client:</b> </p>-->
            
        </div>
    </div><! --/row -->
    
    <br/>
    <div class="row">
        <div class="col-lg-10 col-lg-offset-1">
            <h4>Related Publications</h4>
            <ul class="bibliography"><li><br />

<ul class="reference">
    <li class="pub-title">
        <b>Open Challenges of Blind People using Smartphones</b>
        
        
    </li>
    <li>Rodrigues, André and Nicolau, Hugo and Montague, Kyle and Guerreiro, João and Guerreiro, Tiago</li> <!-- TODO order of names -->
    <li><i>International Journal of Human-Computer Studies, , 2020</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Rodrigues-IJHCS-2020')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2020/Rodrigues-IJHCS-2020.pdf">PDF</a>]
        
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Rodrigues-IJHCS-2020" class="pub-abstract"><p>Blind people face significant challenges when using smartphones. The focus on improving non-visual mobile accessibility has been at the level of touchscreen access. Our research investigates the challenges faced by blind people in their everyday usage of mobile phones. In this paper, we present a set of studies performed with the target population, novices and experts, using a variety of methods, targeted at identifying and verifying challenges; and coping mechanisms. Through a multiple methods approach we identify and validate challenges locally with a diverse set of user expertise and devices, and at scale through the analyses of the largest Android and iOS dedicate forums for blind people. We contribute with a comprehensive corpus of smartphone challenges for blind people, an assessment of their perceived relevance for users with different expertise levels, and a discussion on a set of directions for future research that tackle the open and often overlooked challenges.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>Understanding the Authoring and Playthrough of Nonvisual Smartphone Tutorials</b>
        
        
    </li>
    <li>Rodrigues, André and Santos, André and Montague, Kyle and Nicolau, Hugo and Guerreiro, Tiago</li> <!-- TODO order of names -->
    <li><i>Human-Computer Interaction – INTERACT 2019, 42–62, 2019</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Rodrigues-INTERACT-2019')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2019/Rodrigues-INTERACT-2019.pdf">PDF</a>]
        [<a href="https://link.springer.com/chapter/10.1007/978-3-030-29381-9_4">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Rodrigues-INTERACT-2019" class="pub-abstract"><p>Mobile device users are required to constantly learn to use new apps, features, and adapt to updates. For blind people, adapting to a new interface requires additional time and effort. At the limit, and often so, devices and applications may become unusable without support from someone else. Using tutorials is a common approach to foster independent learning of new concepts and workflows. However, most tutorials available online are limited in scope, detail, or quickly become outdated. Also, they presume a degree of tech savviness that is not at the reach of the common mobile device user. Our research explores the democratization of assistance by enabling non-technical people to create tutorials in their mobile phones for others. We report on the interaction and information needs of blind people when following ‘amateur’ tutorials. Thus, providing insights into how to widen and improve the authoring and playthrough of these learning artifacts. We conducted a study where 12 blind users followed tutorials previously created by blind or sighted people. Our findings suggest that instructions authored by sighted and blind people are limited in different aspects, and that those limitations prevent effective learning of the task at hand. We identified the types of contents produced by authors and the information required by followers during playthrough, which often do not align. We provide insights on how to support both authoring and playthrough of nonvisual smartphone tutorials. There is an opportunity to design solutions that mediate authoring, combine contributions, adapt to user profile, react to context and are living artifacts capable of perpetual improvement.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>Aidme: Interactive Non-visual Smartphone Tutorials</b>
        
        
    </li>
    <li>Rodrigues, André and Camacho, Leonardo and Nicolau, Hugo and Montague, Kyle and Guerreiro, Tiago</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct, 205–212, 2018</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Rodrigues-MobileHCI-2018')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2018/Rodrigues-MobileHCI-2018.pdf">PDF</a>]
        [<a href="http://doi.acm.org/10.1145/3236112.3236141">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Rodrigues-MobileHCI-2018" class="pub-abstract"><p>The constant barrage of updates and novel applications to explore creates a ceaseless cycle of new layouts and interaction methods that we must adapt to. One way to address these challenges is through in-context interactive tutorials. Most applications provide onboarding tutorials using visual metaphors to guide the user through the core features available. However, these tutorials are limited in their scope and are often inaccessible to blind people. In this paper, we present AidMe, a system-wide authoring and playthrough of non-visual interactive tutorials. Tutorials are created via user demonstration and narration. Using AidMe, in a user study with 11 blind participants we identified issues with instruction delivery and user guidance providing insights into the development of accessible interactive non-visual tutorials.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>In-context Q&A to Support Blind People Using Smartphones</b>
        
        
    </li>
    <li>Rodrigues, André and Montague, Kyle and Nicolau, Hugo and Guerreiro, João and Guerreiro, Tiago</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility, , 2017</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Rodrigues-ASSETS-2017')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2017/Rodrigues-ASSETS-2017.pdf">PDF</a>]
        
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Rodrigues-ASSETS-2017" class="pub-abstract"><p>Blind people face many barriers using smartphones. Still, previous research has been mostly restricted to non-visual gestural interaction, paying little attention to the deeper daily challenges of blind users. To bridge this gap, we conducted a series of workshops with 42 blind participants, uncovering application challenges across all levels of expertise, most of which could only be surpassed through a support network. We propose Hint Me!, a human-powered service that allows blind users to get in-app assistance by posing questions or browsing previously answered questions on a shared knowledge-base. We evaluated the perceived usefulness and acceptance of this approach with six blind people. Participants valued the ability to learn independently and anticipated a series of usages: labeling, layout and feature descriptions, bug workarounds, and learning to accomplish tasks. Creating or browsing questions depends on aspects like privacy, knowledge of respondents and response time, revealing the benefits of a hybrid approach.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>Investigating Laboratory and Everyday Typing Performance of Blind Users</b>
        
        
    </li>
    <li>Nicolau, Hugo and Montague, Kyle and Guerreiro, Tiago and Rodrigues, André and Hanson, Vicki L.</li> <!-- TODO order of names -->
    <li><i>ACM Trans. Access. Comput., 4:1–4:26, 2017</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Nicolau-TACCESS-2017')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2017/Nicolau-TACCESS-2017.pdf">PDF</a>]
        [<a href="http://doi.acm.org/10.1145/3046785">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Nicolau-TACCESS-2017" class="pub-abstract"><p>Over the last decade there have been numerous studies on touchscreen typing by blind people. However, there are no reports about blind users’ everyday typing performance and how it relates to laboratory settings. We conducted a longitudinal study involving five participants to investigate how blind users truly type on their smartphones. For twelve weeks, we collected field data, coupled with eight weekly laboratory sessions. This paper provides a thorough analysis of everyday typing data and its relationship with controlled laboratory assessments. We improve state-of-the-art techniques to obtain intent from field data, and provide insights on real-world performance. Our findings show that users improve over time, even though it is at a slow rate. Substitutions are the most common type of error and have a significant impact on entry rates in both field and laboratory settings. Results show that participants are 1.3-2 times faster when typing during everyday tasks. On the other hand, they are less accurate. We finished by deriving some implications that should inform the design of future virtual keyboard for non-visual input. Moreover, findings should be of interest to keyboard designers and researchers looking to conduct field studies to understand everyday input performance.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>Typing Performance of Blind Users: An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage</b>
        
        
    </li>
    <li>Nicolau, Hugo and Montague, Kyle and Guerreiro, Tiago and Rodrigues, André and Hanson, Vicki L.</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 17th International ACM SIGACCESS Conference on Computers &#38; Accessibility, 273–280, 2015</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Nicolau-ASSETS-2015')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2015/Nicolau-ASSETS-2015.pdf">PDF</a>]
        [<a href="http://doi.acm.org/10.1145/2700648.2809861">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Nicolau-ASSETS-2015" class="pub-abstract"><p>Non-visual text-entry for people with visual impairments has focused mostly on the comparison of input techniques reporting on performance measures, such as accuracy and speed. While researchers have been able to establish that non-visual input is slow and error prone, there is little understanding on how to improve it. To develop a richer characterization of typing performance, we conducted a longitudinal study with five novice blind users. For eight weeks, we collected in-situ usage data and conducted weekly laboratory assessment sessions. This paper presents a thorough analysis of typing performance that goes beyond traditional aggregated measures of text-entry and reports on character-level errors and touch measures. Our findings show that users improve over time, even though it is at a slow rate (0.3 WPM per week). Substitutions are the most common type of error and have a significant impact on entry rates. In addition to text input data, we analyzed touch behaviors, looking at touch contact points, exploration movements, and lift positions. We provide insights on why and how performance improvements and errors occur. Finally, we derive some implications that should inform the design of future virtual keyboards for non-visual input</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>Getting Smartphones to Talkback: Understanding the Smartphone Adoption Process of Blind Users</b>
        
        
    </li>
    <li>Rodrigues, André and Montague, Kyle and Nicolau, Hugo and Guerreiro, Tiago</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 17th International ACM SIGACCESS Conference on Computers &#38; Accessibility, 23–32, 2015</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Rodrigues-ASSETS-2015')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2015/Rodrigues-ASSETS-2015.pdf">PDF</a>]
        [<a href="http://doi.acm.org/10.1145/2700648.2809842">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Rodrigues-ASSETS-2015" class="pub-abstract"><p>The advent of system-wide accessibility services on mainstream touch-based smartphones has been a major point of inclusion for blind and visually impaired people. Ever since, researchers aimed to improve the accessibility of specific tasks, such text-entry and gestural interaction. However, little work aimed to understand and improve the overall accessibility of these devices in real world settings. In this paper, we present an eight-week long study with five novice blind participants where we seek to understand major concerns, expectations, challenges, barriers, and experiences with smartphones. The study included pre-adoption and weekly interviews, weekly controlled task assessments, and in-the wild system-wide usage. Our results show that mastering these devices is an arduous and long task, confirming the users’ initial concerns. We report on accessibility barriers experienced throughout the study, which could not be encountered in task-based laboratorial settings. Finally, we discuss how smartphones are being integrated in everyday activities and highlight the need for better adoption support tools.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>TinyBlackBox: Supporting Mobile In-The-Wild Studies</b>
        
        
    </li>
    <li>Montague, Kyle and Rodrigues, André and Nicolau, Hugo and Guerreiro, Tiago</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 17th International ACM SIGACCESS Conference on Computers &#38; Accessibility, 379–380, 2015</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Montague-ASSETS-2015')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2015/Montague-ASSETS-2015.pdf">PDF</a>]
        [<a href="http://doi.acm.org/10.1145/2700648.2811379">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Montague-ASSETS-2015" class="pub-abstract"><p>Most work investigating mobile HCI is carried out within controlled laboratory settings; these spaces are not representative of the real-world environments for which the technology will predominantly be used. The result of which can produce a skewed or inaccurate understanding of interaction behaviors and users’ abilities. While mobile in-the-wild studies provide more realistic representations of technology usage, there are additional challenges to conducting data collection outside of the lab. In this paper we discuss these challenges and present TinyBlackBox, a standalone data collection framework to support mobile in-thewild studies with today’s smartphone and tablet devices.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>Motor-impaired Touchscreen Interactions in the Wild</b>
        
        
    </li>
    <li>Montague, Kyle and Nicolau, Hugo and Hanson, Vicki L.</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 16th International ACM SIGACCESS Conference on Computers & Accessibility, 123–130, 2014</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Montague-ASSETS-2014')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2014/Montague-ASSETS-2014.pdf">PDF</a>]
        [<a href="http://doi.acm.org/10.1145/2661334.2661362">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Montague-ASSETS-2014" class="pub-abstract"><p>Touchscreens are pervasive in mainstream technologies; they offer novel user interfaces and exciting gestural interactions. However, to interpret and distinguish between the vast ranges of gestural inputs, the devices require users to consistently perform interactions inline with the predefined location, movement and timing parameters of the gesture recognizers. For people with variable motor abilities, particularly hand tremors, performing these input gestures can be extremely challenging and impose limitations on the possible interactions the user can make with the device. In this paper, we examine touchscreen performance and interaction behaviors of motor-impaired users on mobile devices. The primary goal of this work is to measure and understand the variance of touchscreen interaction performances by people with motor-impairments. We conducted a four-week in-the-wild user study with nine participants using a mobile touchscreen device. A Sudoku stimulus application measured their interaction performance abilities during this time. Our results show that not only does interaction performance vary significantly between users, but also that an individual’s interaction abilities are significantly different between device sessions. Finally, we propose and evaluate the effect of novel tap gesture recognizers to accommodate for individual variances in touchscreen interactions.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>NavTap: A Long Term Study with Excluded Blind Users</b>
        
        
    </li>
    <li>Guerreiro, Tiago and Nicolau, Hugo and Jorge, Joaquim and Gonçalves, Daniel</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 11th International ACM SIGACCESS Conference on Computers and Accessibility, 99–106, 2009</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Guerreiro-ASSETS-2009')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2009/Guerreiro-ASSETS-2009.pdf">PDF</a>]
        [<a href="http://doi.acm.org/10.1145/1639642.1639661">LIBRARY</a>]
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Guerreiro-ASSETS-2009" class="pub-abstract"><p>NavTap is a navigational method that enables blind users to input text in a mobile device by reducing the associated cognitive load. In this paper, we present studies that go beyond a laboratorial setting, exploring the methods’ effectiveness and learnability as well as its influence on the users’ daily lives. Eight blind users participated in designing the prototype (3 weeks) while five took part in the studies along 16 more weeks. Results gathered in controlled weekly sessions and real life usage logs enabled us to better understand NavTap’s advantages and limitations. The method revealed itself both as easy to learn and improve. Indeed, users were able to better control their mobile devices to send SMS and use other tasks that require text input such as managing a phonebook, from day one, in real-life settings. While individual user profiles play an important role in determining their evolution, even less capable users (with ageinduced impairments or cognitive difficulties), were able to perform the assigned tasks (sms, directory) both in the laboratory and in everyday use, showing continuous improvement to their skills. According to interviews, none were able to input text before. Nav-Tap dramatically changed their relation with mobile devices and noticeably improved their social interaction capabilities.</p><div class="hline"></div></div>
</ul>

</li>
<li><br />

<ul class="reference">
    <li class="pub-title">
        <b>NavTap: Um Estudo de Longa Duracao com Utilizadores Cegos</b>
        <img class="pub-flag" src="../assets/img/pt.gif" alt="portuguese flag"/> 
        <span class="pub-award">Best Student Paper Award</span>
    </li>
    <li>Guerreiro, Tiago and Nicolau, Hugo and Jorge, Joaquim and Gonçalves, Daniel</li> <!-- TODO order of names -->
    <li><i>Proceedings of the 17th Encontro Portugues de Computacao Grafica (EPCG), , 2009</i></li>
    <li>
        [<a class="pub-abstract-link" onClick="toggleVisible('Nicolau-EPCG-2009')">ABSTRACT</a>]
        [<a href="http://web.tecnico.ulisboa.pt/hugo.nicolau/publications/2009/Nicolau-EPCG-2009.pdf">PDF</a>]
        
        
        <!-- TODO toggle bibtext -->
    </li>
    <div id="Nicolau-EPCG-2009" class="pub-abstract"><p>NavTap is a navigational method that enables blind users to input text in a mobile device by reducing the associated cognitive load. We present studies that go beyond a laboratorial setting, exploring the methods’ effectiveness and learnability as well as its influence in the users’ daily lives. Eight blind users participated in the prototype’s design (3 weeks) while five took part in the studies along 16 more weeks. All were unable to input text before. Results gathered in controlled weekly sessions and real life interaction logs revealed the method as easy to learn and improve performance, as the users were able to fully control mobile devices in the first contact within real life scenarios. The individual profiles play an important role determining evolution and even less capable users (with age-induced impairments or cognitive difficulties) were able to perform the required tasks, in and out of the laboratory, with continuous improvements. NavTap dramatically changed the users’ relation with the devices and improved their social interaction capabilities. </p><div class="hline"></div></div>
</ul>

</li></ul>
        </div>
    </div>
</div><! --/container -->

        <div id="footerwrap">
    <div class="container">
        <div class="row">
            <div class="col-lg-4">
                <h4>About</h4>
                <div class="hline-w"></div>
                <p>Hugo is an Assistant Professor in the <a href='https://fenix.tecnico.ulisboa.pt/departamentos/dei'>Computer Science and Engineering Department (DEI)</a> of <a href='http://tecnico.ulisboa.pt/en/'>Instituto Superior Técnico</a>, <a href='http://www.ulisboa.pt/en/'>University of Lisbon</a> in Portugal. He's also a researcher at the <a href='https://vimmi.inesc-id.pt/'>Visualization and Intelligent Multimodal Interfaces (VIMMI)</a> group at <a href='http://inesc-id.pt/'>INESC-ID</a>.</p>
            </div>
            <div class="col-lg-4">
                <h4>Social Links</h4>
                <div class="hline-w"></div>
                <p>
                    
                        <a href="https://www.facebook.com/hugo.nicolau" class="btn-social btn-outline"><i class="fa fa-facebook"></i></a>
                    
                        <a href="http://twitter.com/hnicolau" class="btn-social btn-outline"><i class="fa fa-twitter"></i></a>
                    
                        <a href="http://pt.linkedin.com/in/hugonicolau" class="btn-social btn-outline"><i class="fa fa-linkedin"></i></a>
                    
                        <a href="https://github.com/hnicolau" class="btn-social btn-outline"><i class="fa fa-github"></i></a>
                    
                </p>
            </div>
            <div class="col-lg-4">
                <h4>My Office</h4>
                <div class="hline-w"></div>
                <p>
                    
                        Instituto Superior Técnico, <br>
                    
                        Computer Science and Engineering Department, <br>
                    
                        Room 2-N9.21 - TagusPark <br>
                    
                        Avenida Professor Cavaco Silva, <br>
                    
                        2744-016 Porto Salvo, <br>
                    
                        Portugal <br>
                    
                </p>
            </div>
        </div><! --/row -->
    </div><! --/container -->
</div><! --/footerwrap -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/js/bootstrap.min.js"></script>
<script src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/js/retina-1.1.0.js"></script>
<script src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/js/jquery.hoverdir.js"></script>
<script src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/js/jquery.hoverex.min.js"></script>
<script src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/js/jquery.prettyPhoto.js"></script>
<script src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/js/jquery.isotope.min.js"></script>
<script src="http://web.tecnico.ulisboa.pt/hugo.nicolau/assets/js/custom.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script>

<script>
// Portfolio
(function($) {
	"use strict";
	var $container = $('.portfolio'),
		$items = $container.find('.portfolio-item'),
		portfolioLayout = 'fitRows';
		
		if( $container.hasClass('portfolio-centered') ) {
			portfolioLayout = 'masonry';
		}
				
		$container.isotope({
			filter: '*',
			animationEngine: 'best-available',
			layoutMode: portfolioLayout,
			animationOptions: {
			duration: 750,
			easing: 'linear',
			queue: false
		},
		masonry: {
		}
		}, refreshWaypoints());
		
		function refreshWaypoints() {
			setTimeout(function() {
			}, 1000);   
		}
				
		$('nav.portfolio-filter ul a').on('click', function() {
				var selector = $(this).attr('data-filter');
				$container.isotope({ filter: selector }, refreshWaypoints());
				$('nav.portfolio-filter ul a').removeClass('active');
				$(this).addClass('active');
				return false;
		});
		
		function getColumnNumber() { 
			var winWidth = $(window).width(), 
			columnNumber = 1;
		
			if (winWidth > 1200) {
				columnNumber = 5;
			} else if (winWidth > 950) {
				columnNumber = 4;
			} else if (winWidth > 600) {
				columnNumber = 3;
			} else if (winWidth > 400) {
				columnNumber = 2;
			} else if (winWidth > 250) {
				columnNumber = 1;
			}
				return columnNumber;
			}       
			
			function setColumns() {
				var winWidth = $(window).width(), 
				columnNumber = getColumnNumber(), 
				itemWidth = Math.floor(winWidth / columnNumber);
				
				$container.find('.portfolio-item').each(function() { 
					$(this).css( { 
					width : itemWidth + 'px' 
				});
			});
		}
		
		function setPortfolio() { 
			setColumns();
			$container.isotope('reLayout');
		}
			
		$container.imagesLoaded(function () { 
			setPortfolio();
		});
		
		$(window).on('resize', function () { 
		setPortfolio();          
	});
})(jQuery);
</script>

<script>
    //bibliography
    function toggleVisible(id) {    
        var el = document.getElementById(id); 
        console.log(el.style.display);
        if (el.style.display == "block") {      
            el.style.display = "none";
        } 
        else {
            el.style.display = "block";
        }
    }

</script>
    </body>
</html>